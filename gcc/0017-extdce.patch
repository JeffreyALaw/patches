diff --git a/gcc/ext-dce.cc b/gcc/ext-dce.cc
index 6d4b8858ec6..c4c38659701 100644
--- a/gcc/ext-dce.cc
+++ b/gcc/ext-dce.cc
@@ -48,6 +48,44 @@ static bool modify;
    bit 16..31
    bit 32..BITS_PER_WORD-1  */
 
+static int
+group_limit (const_rtx reg)
+{
+  machine_mode mode = GET_MODE (reg);
+
+  if (!GET_MODE_BITSIZE (mode).is_constant ())
+    return 3;
+
+  int size = GET_MODE_SIZE (mode).to_constant ();
+
+  size = exact_log2 (size);
+
+  if (size < 0)
+    return 3;
+
+  return (size > 3 ? 3 : size);
+}
+
+/* Given the bit tracking shown above, make all bit groups live
+   for REGNO in bitmap BMAP.  For hard regs, we assume all groups are
+   live.  For a pseudo we consider the size of the pseudo to avoid
+   creating unnecessarily live chunks of data.  */
+
+static void
+make_reg_live (bitmap bmap, int regno)
+{
+  int limit;
+  if (regno > FIRST_PSEUDO_REGISTER)
+    {
+      rtx reg = regno_reg_rtx[regno];
+      limit = group_limit (reg) + 1;
+    }
+  else
+    limit = 4;
+  for (int i = 0; i < limit; i++)
+    bitmap_set_bit (bmap, regno * 4 + i);
+}
+
 /* Note this pass could be used to narrow memory loads too.  It's
    not clear if that's profitable or not in general.  */
 
@@ -69,6 +107,7 @@ safe_for_live_propagation (rtx_code code)
   switch (GET_RTX_CLASS (code))
     {
       case RTX_OBJ:
+      case RTX_CONST_OBJ:
 	return true;
 
       case RTX_COMPARE:
@@ -129,9 +168,11 @@ safe_for_live_propagation (rtx_code code)
    within an object) are set by INSN, the more aggressive the
    optimization phase during use handling will be.  */
 
-static void
+static bool
 ext_dce_process_sets (rtx_insn *insn, rtx obj, bitmap live_tmp)
 {
+  bool skipped_dest = false;
+
   subrtx_iterator::array_type array;
   FOR_EACH_SUBRTX (iter, array, obj, NONCONST)
     {
@@ -158,6 +199,7 @@ ext_dce_process_sets (rtx_insn *insn, rtx obj, bitmap live_tmp)
 	      /* Skip the subrtxs of this destination.  There is
 		 little value in iterating into the subobjects, so
 		 just skip them for a bit of efficiency.  */
+	      skipped_dest = true;
 	      iter.skip_subrtxes ();
 	      continue;
 	    }
@@ -189,16 +231,34 @@ ext_dce_process_sets (rtx_insn *insn, rtx obj, bitmap live_tmp)
 		  /* Skip the subrtxs of the STRICT_LOW_PART.  We can't
 		     process them because it'll set objects as no longer
 		     live when they are in fact still live.  */
+		  skipped_dest = true;
 		  iter.skip_subrtxes ();
 		  continue;
 		}
 
-	      /* Transfer all the LIVENOW bits for X into LIVE_TMP.  */
+	      /* LIVE_TMP contains the set groups that are live-out
+		 and set in this insn.  It is used to narrow the groups
+		 live-in for the inputs of this insn.
+
+		 The simple thing to do is mark all the groups as live,
+		 but that will significantly inhibit optimization.
+
+		 We also need to be careful for of the case where we have
+		 an in-out operand.  If we're not careful we'd clear
+		 LIVE_TMP in that case which will cause problems later.
+
+		 The right thing to is to transfer the live groups from
+		 LIVENOW into LIVE_TMP.  If LIVE_TMP is empty afterwards,
+		 then set it based on the REG and its mode.  */
 	      HOST_WIDE_INT rn = REGNO (SUBREG_REG (x));
-	      for (HOST_WIDE_INT i = 4 * rn; i < 4 * rn + 4; i++)
+	      int limit = group_limit (SUBREG_REG (x)) + 1;
+	      for (HOST_WIDE_INT i = 4 * rn; i < 4 * rn + limit; i++)
 		if (bitmap_bit_p (livenow, i))
 		  bitmap_set_bit (live_tmp, i);
 
+	      if (bitmap_empty_p (live_tmp))
+		bitmap_set_range (live_tmp, 4 * rn, limit);
+
 	      /* The mode of the SUBREG tells us how many bits we can
 		 clear.  */
 	      machine_mode mode = GET_MODE (x);
@@ -217,11 +277,19 @@ ext_dce_process_sets (rtx_insn *insn, rtx obj, bitmap live_tmp)
 	    = GET_MODE_MASK (GET_MODE_INNER (GET_MODE (x)));
 	  if (SUBREG_P (x))
 	    {
-	      /* If we have a SUBREG that is too wide, just continue the loop
-		 and let the iterator go down into SUBREG_REG.  */
+	      /* If we have a SUBREG destination that is too wide, just
+		 skip the destination rather than continuing this iterator.
+		 While continuing would be better, we'd need to strip the
+		 subreg and restart within the SET processing rather than
+		 the top of the loop which just complicates the flow even
+		 more.  */
 	      if (!is_a <scalar_int_mode> (GET_MODE (SUBREG_REG (x)), &outer_mode)
 		  || GET_MODE_BITSIZE (outer_mode) > 64)
-		continue;
+		{
+		  skipped_dest = true;
+		  iter.skip_subrtxes ();
+		  continue;
+		}
 
 	      /* We can safely strip a paradoxical subreg.  The inner mode will
 		 be narrower than the outer mode.  We'll clear fewer bits in
@@ -246,6 +314,7 @@ ext_dce_process_sets (rtx_insn *insn, rtx obj, bitmap live_tmp)
 		 remain the same.  Thus we can not continue here, we must
 		 either figure out what part of the destination is modified
 		 or skip the sub-rtxs.  */
+	      skipped_dest = true;
 	      iter.skip_subrtxes ();
 	      continue;
 	    }
@@ -256,13 +325,20 @@ ext_dce_process_sets (rtx_insn *insn, rtx obj, bitmap live_tmp)
 	  /* Now handle the actual object that was changed.  */
 	  if (REG_P (x))
 	    {
-	      /* Transfer the appropriate bits from LIVENOW into
-		 LIVE_TMP.  */
+	      /* Mark all the potentially set groups as live.  This must not
+		 filter based on LIVE_TMP as we might have a matched source
+		 and destination.  If the source wasn't already considered
+		 live, then its groups wouldn't be marked in LIVE_TMP and
+		 subsequent use processing would think all the bits in the
+		 destination are dead.  */
 	      HOST_WIDE_INT rn = REGNO (x);
-	      for (HOST_WIDE_INT i = 4 * rn; i < 4 * rn + 4; i++)
+	      for (HOST_WIDE_INT i = 4 * rn; i < 4 * rn + group_limit (x) + 1; i++)
 		if (bitmap_bit_p (livenow, i))
 		  bitmap_set_bit (live_tmp, i);
 
+	      if (bitmap_empty_p (live_tmp))
+		bitmap_set_range (live_tmp, 4 * rn, group_limit (x) + 1);
+
 	      /* Now clear the bits known written by this instruction.
 		 Note that BIT need not be a power of two, consider a
 		 ZERO_EXTRACT destination.  */
@@ -286,9 +362,11 @@ ext_dce_process_sets (rtx_insn *insn, rtx obj, bitmap live_tmp)
       else if (GET_CODE (x) == COND_EXEC)
 	{
 	  /* This isn't ideal, but may not be so bad in practice.  */
+	  skipped_dest = true;
 	  iter.skip_subrtxes ();
 	}
     }
+  return skipped_dest;
 }
 
 /* INSN has a sign/zero extended source inside SET that we will
@@ -449,7 +527,12 @@ carry_backpropagate (unsigned HOST_WIDE_INT mask, enum rtx_code code, rtx x)
       return mmask;
 
     case SIGN_EXTEND:
-      if (mask & ~GET_MODE_MASK (GET_MODE_INNER (GET_MODE (XEXP (x, 0)))))
+      /* If there are bits live outside the mode of the object being
+	 extended, then they would be copies of the sign bit of the
+	 object being extended.  So make sure the sign bit of the
+	 object being extended is live in that case.  */
+      mode = GET_MODE_INNER (GET_MODE (XEXP (x, 0)));
+      if (mask & ~GET_MODE_MASK (mode))
 	mask |= 1ULL << (GET_MODE_BITSIZE (mode).to_constant () - 1);
       return mask;
 
@@ -482,7 +565,8 @@ carry_backpropagate (unsigned HOST_WIDE_INT mask, enum rtx_code code, rtx x)
    eliminated in CHANGED_PSEUDOS.  */
 
 static void
-ext_dce_process_uses (rtx_insn *insn, rtx obj, bitmap live_tmp)
+ext_dce_process_uses (rtx_insn *insn, rtx obj,
+		      bitmap live_tmp, bool skipped_dest)
 {
   subrtx_var_iterator::array_type array_var;
   FOR_EACH_SUBRTX_VAR (iter, array_var, obj, NONCONST)
@@ -556,6 +640,11 @@ ext_dce_process_uses (rtx_insn *insn, rtx obj, bitmap live_tmp)
 		  dst_mask |= mask_array[i];
 	      dst_mask >>= bit;
 
+	      /* If we ignored the destination during set processing, then
+		 consider all the bits live.  */
+	      if (skipped_dest)
+		dst_mask = -1;
+
 	      /* ??? Could also handle ZERO_EXTRACT / SIGN_EXTRACT
 		 of the source specially to improve optimization.  */
 	      if (code == SIGN_EXTEND || code == ZERO_EXTEND)
@@ -566,7 +655,7 @@ ext_dce_process_uses (rtx_insn *insn, rtx obj, bitmap live_tmp)
 
 		  /* DST_MASK could be zero if we had something in the SET
 		     that we couldn't handle.  */
-		  if (modify && dst_mask && (dst_mask & ~src_mask) == 0)
+		  if (modify && !skipped_dest && (dst_mask & ~src_mask) == 0)
 		    ext_dce_try_optimize_insn (insn, x);
 
 		  dst_mask &= src_mask;
@@ -574,10 +663,8 @@ ext_dce_process_uses (rtx_insn *insn, rtx obj, bitmap live_tmp)
 		  code = GET_CODE (src);
 		}
 
-	      /* Optimization is done at this point.  We just want to make
-		 sure everything that should get marked as live is marked
-		 from here onward.  */
-
+	      /* Make sure to account for carry_backpropagate before doing
+		 optimization!  */
 	      dst_mask = carry_backpropagate (dst_mask, code, src);
 
 	      /* We will handle the other operand of a binary operator
@@ -591,8 +678,10 @@ ext_dce_process_uses (rtx_insn *insn, rtx obj, bitmap live_tmp)
 		 making things live.  Breaking from this loop will cause
 		 the iterator to work on sub-rtxs, so it is safe to break
 		 if we see something we don't know how to handle.  */
+	      unsigned HOST_WIDE_INT save_mask = dst_mask;
 	      for (;;)
 		{
+		  dst_mask = save_mask;
 		  /* Strip an outer paradoxical subreg.  The bits outside
 		     the inner mode are don't cares.  So we can just strip
 		     and process the inner object.  */
@@ -689,7 +778,7 @@ ext_dce_process_uses (rtx_insn *insn, rtx obj, bitmap live_tmp)
       /* If we have a register reference that is not otherwise handled,
 	 just assume all the chunks are live.  */
       else if (REG_P (x))
-	bitmap_set_range (livenow, REGNO (x) * 4, 4);
+	bitmap_set_range (livenow, REGNO (x) * 4, group_limit (x) + 1);
     }
 }
 
@@ -716,14 +805,15 @@ ext_dce_process_bb (basic_block bb)
       bitmap live_tmp = BITMAP_ALLOC (NULL);
 
       /* First process any sets/clobbers in INSN.  */
-      ext_dce_process_sets (insn, PATTERN (insn), live_tmp);
+      bool skipped_dest = ext_dce_process_sets (insn, PATTERN (insn), live_tmp);
 
       /* CALL_INSNs need processing their fusage data.  */
       if (CALL_P (insn))
-	ext_dce_process_sets (insn, CALL_INSN_FUNCTION_USAGE (insn), live_tmp);
+	skipped_dest |= ext_dce_process_sets (insn, CALL_INSN_FUNCTION_USAGE (insn),
+					      live_tmp);
 
       /* And now uses, optimizing away SIGN/ZERO extensions as we go.  */
-      ext_dce_process_uses (insn, PATTERN (insn), live_tmp);
+      ext_dce_process_uses (insn, PATTERN (insn), live_tmp, skipped_dest);
 
       /* A nonlocal goto implicitly uses the frame pointer.  */
       if (JUMP_P (insn) && find_reg_note (insn, REG_NON_LOCAL_GOTO, NULL_RTX))
@@ -746,7 +836,8 @@ ext_dce_process_bb (basic_block bb)
 	      if (global_regs[i])
 		bitmap_set_range (livenow, i * 4, 4);
 
-	  ext_dce_process_uses (insn, CALL_INSN_FUNCTION_USAGE (insn), live_tmp);
+	  ext_dce_process_uses (insn, CALL_INSN_FUNCTION_USAGE (insn),
+				live_tmp, false);
 	}
 
       BITMAP_FREE (live_tmp);
@@ -816,10 +907,7 @@ ext_dce_init (void)
   unsigned i;
   bitmap_iterator bi;
   EXECUTE_IF_SET_IN_BITMAP (refs, 0, i, bi)
-    {
-      for (int j = 0; j < 4; j++)
-	bitmap_set_bit (&livein[EXIT_BLOCK], i * 4 + j);
-    }
+    make_reg_live (&livein[EXIT_BLOCK], i);
 
   livenow = BITMAP_ALLOC (NULL);
   all_blocks = BITMAP_ALLOC (NULL);
@@ -875,8 +963,6 @@ ext_dce_rd_transfer_n (int bb_index)
      the generic dataflow code that something changed.  */
   if (!bitmap_equal_p (&livein[bb_index], livenow))
     {
-      gcc_assert (!bitmap_intersect_compl_p (&livein[bb_index], livenow));
-
       bitmap_copy (&livein[bb_index], livenow);
       return true;
     }
